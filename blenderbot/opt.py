# params = {
#     'embedding_size': 2560,
#     'n_positions': 128,
#     'n_segments': 0,
#     'n_encoder_layers': 2,
#     'n_decoder_layers': 16,
#     'n_heads': 32,
#     'ffn_size': 10240,
#     'dropout': 0.1,
#     'attention_dropout': 0.0,
#     'relu_dropout': 0.0,
#     'learn_positional_embeddings': False,
#     'embeddings_scale': True,
#     'activation': 'gelu',
#     'variant': 'prelayernorm',
#     'output_scaling': 1.0,
# }
params = {
    'embedding_size': 512,
    'n_positions': 512,
    'n_segments': 0,
    'n_encoder_layers': 8,
    'n_decoder_layers': 8,
    'n_heads': 16,
    'ffn_size': 2048,
    'dropout': 0.1,
    'attention_dropout': 0.0,
    'relu_dropout': 0.0,
    'learn_positional_embeddings': True,
    'embeddings_scale': True,
    'activation': 'gelu',
    'variant': 'xlm',
    'output_scaling': 1.0,
}

inference_params = {
    'method': 'beam',
    'beam_size': 10,
    'min_beam_length': 20,
    'beam_block_ngram': 3,
    'beam_context_block_ngram': 3,
    'beam_length_penalty': 0.65,
    'input_maxlen': 128,
    'max_len': 30,
    'topk': 10,
    'beam_delay': 30,
}
