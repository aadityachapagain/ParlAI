{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from transformers import BlenderbotSmallTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BlenderbotSmallTokenizer.from_pretrained('facebook/blenderbot-90M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_msg(txt, ignore_fields=''):\n",
    "    \"\"\"\n",
    "    Convert formatted string to ParlAI message dict.\n",
    "\n",
    "    :param txt:\n",
    "        formatted string to convert. String format is tab-separated fields,\n",
    "        with colon separating field name and contents.\n",
    "    :param ignore_fields:\n",
    "        (default '') comma-separated field names to not\n",
    "        include in the msg dict even if they're in the string.\n",
    "    \"\"\"\n",
    "\n",
    "    def tostr(txt):\n",
    "        txt = str(txt)\n",
    "        txt = txt.replace('\\\\t', '\\t')\n",
    "        txt = txt.replace('\\\\n', '\\n')\n",
    "        txt = txt.replace('__PIPE__', '|')\n",
    "        return txt\n",
    "\n",
    "    def tolist(txt):\n",
    "        vals = txt.split('|')\n",
    "        for v in vals:\n",
    "            v = tostr(v)\n",
    "        return vals\n",
    "\n",
    "    def convert(key, value):\n",
    "        if key == 'text' or key == 'id':\n",
    "            return tostr(value)\n",
    "        elif (\n",
    "            key == 'label_candidates'\n",
    "            or key == 'labels'\n",
    "            or key == 'eval_labels'\n",
    "            or key == 'text_candidates'\n",
    "        ):\n",
    "            return tolist(value)\n",
    "        elif key == 'episode_done':\n",
    "            return bool(value)\n",
    "        else:\n",
    "            return tostr(value)\n",
    "\n",
    "    if txt == '' or txt is None:\n",
    "        return None\n",
    "\n",
    "    msg = {}\n",
    "    for t in txt.split('\\t'):\n",
    "        ind = t.find(':')\n",
    "        key = t[:ind]\n",
    "        value = t[ind + 1 :]\n",
    "        if key not in ignore_fields.split(','):\n",
    "            msg[key] = convert(key, value)\n",
    "    msg['episode_done'] = msg.get('episode_done', False)\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParlaiFormatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 parlai_format_data_path,\n",
    "                 tokenizer,\n",
    "                 text_truncate,\n",
    "                 label_truncate=None):\n",
    "        if label_truncate is None:\n",
    "            label_truncate = text_truncate\n",
    "        \n",
    "        with open(parlai_format_data_path) as f:\n",
    "            turns = [str_to_msg(l.strip()) for l in f.readlines()]\n",
    "            \n",
    "        self.data = []\n",
    "        \n",
    "        history = []\n",
    "        label_tokens = []\n",
    "        for turn in tqdm(turns):\n",
    "            history += label_tokens\n",
    "            for text in turn['text'].split('\\n'):\n",
    "                text_tokens = tokenizer(text,\n",
    "                                        padding=False,\n",
    "                                        return_token_type_ids=False,\n",
    "                                        return_attention_mask=False)['input_ids']\n",
    "                history += text_tokens\n",
    "                history += [tokenizer.eos_token_id,]\n",
    "\n",
    "            label_tokens = tokenizer(turn['labels'][0],\n",
    "                                     padding=False,\n",
    "                                     return_token_type_ids=False,\n",
    "                                     return_attention_mask=False)['input_ids']\n",
    "            label_tokens += [tokenizer.eos_token_id,]\n",
    "\n",
    "            self.data.append((torch.tensor(history[-text_truncate:]), torch.tensor(label_tokens[-label_truncate:])))\n",
    "\n",
    "            if turn['episode_done']:\n",
    "                history = []\n",
    "                label_tokens = []\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collator:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        x, y = zip(*batch)\n",
    "        x = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        y = nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27018/27018 [00:19<00:00, 1418.84it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = ParlaiFormatDataset('data/train.txt',\n",
    "                              tokenizer,\n",
    "                              text_truncate=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True,\n",
    "                                         collate_fn=Collator(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6652"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([data[0].shape[1] == 128 for data in dataloader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6755"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 24])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
